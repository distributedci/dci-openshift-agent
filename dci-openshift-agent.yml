---
# Step 0 : DCI setup
- name: 'Set dci variables'
  hosts: localhost
  tags:
    - job
    - dci
  environment:
    http_proxy: "{{ http_proxy }}"
    https_proxy: "{{ https_proxy }}"
    no_proxy: "{{ no_proxy_list }}"
  tasks:
    - name: Display proxy settings
      debug:
        msg: "http_proxy={{ http_proxy }} https_proxy={{ https_proxy }} no_proxy={{ no_proxy_list }}"

    - name: Read credentials from env vars
      set_fact:
        dci_client_id: "{{ lookup('env','DCI_CLIENT_ID') }}"
        dci_api_secret: "{{ lookup('env','DCI_API_SECRET') }}"
        dci_cs_url: "{{ lookup('env','DCI_CS_URL') }}"
        dci_ui_url: "{{ lookup('env','DCI_UI_URL') | default('https://www.distributed-ci.io', True) }}"
      no_log: "{{ dci_hide_secrets }}"

    # Schedule a new job only if not passed via dci-pipeline
    - name: Schedule a new job
      dci_job:
        components: "{{ dci_components }}"
        components_by_query: "{{ dci_components_by_query }}"
        topic: "{{ dci_topic }}"
        comment: "{{ dci_comment }}"
        url: "{{ dci_url }}"
        name: "{{ dci_name }}"
        configuration: "{{ dci_configuration }}"
        previous_job_id: "{{ dci_previous_job_id }}"
        team_id: "{{ dci_team_id }}"
        pipeline_id: "{{ dci_pipeline_id }}"
      register: job_info
      when: job_info is not defined

    - name: Custom OCP build component
      when:
        - dci_custom_component | default(false) | bool
        - dci_custom_component_file is defined
        - dci_custom_component_file | length
      block:
        - name: Load custom build component
          set_fact:
            custom_component: "{{ lookup('file', dci_custom_component_file) | from_json }}"

    - name: Set the job id
      set_fact:
        job_id: '{{ job_info.job.id }}'

    - name: Copy the job_id to the JOB_ID_FILE if it exists
      copy:
        content: "{{ job_id }}"
        dest: "{{ JOB_ID_FILE }}"
        mode: "0644"
      when: JOB_ID_FILE is defined

    # Keep in sync with test-runner parsing
    - name: UI URL
      debug:
        msg: 'Follow the log at: {{ dci_ui_url }}/jobs/{{ hostvars.localhost.job_info.job.id }}/jobStates' # noqa 204

    - name: Set DCI tags for the current job
      dci_job:
        id: '{{ job_id }}'
        tags: >
          {{
          [ (dci_disconnected | default(false) | bool) | ternary("disconnected", "connected") ] +
          ["agent:openshift"] +
          dci_tags +
          dci_workarounds +
          ((dci_main == "upgrade") |
          ternary(["upgrade"], ["install", "install_type:" + (install_type | default('ipi'))])) }}

    - name: Add cluster tag to the current job
      dci_job:
        id: '{{ job_id }}'
        tags:
          - cluster:{{ cluster }}
      when: cluster is defined

    - name: Count workarounds
      dci_job:
        id: "{{ job_id }}"
        key: "workarounds"
        value: "{{ dci_workarounds | length }}"

    - name: Debug components
      debug:
        msg: "{{ job_info.job.components + [custom_component] if custom_component is defined else job_info.job.components }}"

    - name: Get openshift_version from job.components
      set_fact:
        ocp_tags: "{{ item['tags'] }}"
        url: "{{ item['url'] | regex_replace('\\/$', '') }}"
        version: "{{ item.version }}"
        version_pull_url: "{{ item['data']['pull_url'] }}"
        ocp_product_id: "{{ item['id'] }}"
      with_items: "{{ job_info.job.components + [custom_component] if custom_component is defined else job_info.job.components }}"
      when: item["type"] == "ocp"

    - name: Get build from tag
      set_fact:
        build: "{{ item.split(':')[1] }}"
      with_items: "{{ ocp_tags }}"
      when: item.split(":")[0] == "build"

    # Candidate builds are ga builds that haven't gone ga quite yet.
    # The following changes the build and version vars to look like ga
    # for the upstream install playbooks. The dci-feeder adds '-0.rc-$TIMESTAMP'
    # to the version, we have to strip it here.
    - name: Set build:ga when build:candidate
      set_fact:
        build: "ga"
      when: build == "candidate"

    - name: Pre-run
      dci_job:
        id: "{{ job_id }}"
        status: "pre-run"

    - name: "Assisted on-prem is deprecated"
      ansible.builtin.fail:
        msg:
          - "Assisted on-prem is deprecated"
          - "See Assisted on-prem deprecation at docs/ai-on-prem-deprecation.md for details"
      when:
        - (install_type | default('ipi')) == 'assisted'
        - use_agent_based_installer is defined

    - name: Fail when ABI version is not supported
      ansible.builtin.fail:
        msg: "ABI support starts in 4.12"
      when:
        - version is version('4.12', '<')
        - (install_type | default('ipi')) == 'abi'

    - name: "Requirements for ACM install_type"
      when:
        - (install_type | default('ipi')) == 'acm'
      block:
        - name: "Read HUB_KUBECONFIG path from env vars"
          set_fact:
            hub_kubeconfig_path: "{{ lookup('env','HUB_KUBECONFIG') }}"
          when: hub_kubeconfig_path is not defined

        - name: "Check if HUB_KUBECONFIG path exists"
          stat:
            path: "{{ hub_kubeconfig_path }}"
          register: hub_kubeconfig

        - name: "Fail if hub_kubeconfig NOT found"
          fail:
            msg: "hub_kubeconfig not found at {{ hub_kubeconfig_path }}"
          when: not hub_kubeconfig.stat.exists

# Step 1a : pre run
- name: 'Launch pre-run'
  hosts: localhost
  tags:
    - pre-run
  environment:
    http_proxy: "{{ http_proxy }}"
    https_proxy: "{{ https_proxy }}"
    no_proxy: "{{ no_proxy_list }}"
  tasks:
    - name: Pre-run
      block:
        - name: Configure jumpbox
          include_tasks: plays/configure-jumpbox.yml

        - name: "Setup job_logs directory"
          include_tasks: plays/log-dir.yml
          when: job_logs is undefined

        - name: Run the pre-run
          include_tasks: plays/pre-run.yml
          when:
            - (install_type | default('ipi')) not in ['acm']

        - name: "ACM pre-run"
          include_tasks:
            file: plays/acm-pre-run.yml
            apply:
              environment:
                KUBECONFIG: "{{ hub_kubeconfig_path }}"
              tags:
                - acm-pre-run
          when:
            - (install_type | default('ipi')) == 'acm'

      rescue: &teardown_error
        - name: Run the teardown error handler
          delegate_to: localhost
          ignore_unreachable: true
          block:
            - name: Error
              dci_job:
                id: "{{ hostvars.localhost.job_id }}"
                status: "error"
              tags: [dci]
              delegate_to: localhost

            - name: Teardown hooks
              when: dci_teardown_on_failure|bool
              delegate_to: localhost
              ignore_unreachable: true
              ignore_errors: true
              block:
                - name: Run the teardown hooks
                  include_tasks: "{{ hookdir }}/hooks/teardown.yml"
                  loop: "{{ dci_config_dirs | reverse | list }}"
                  loop_control:
                    loop_var: hookdir

                - name: Run the teardown play
                  include_tasks: plays/teardown.yml

          always:
            - name: Run the error process
              include_tasks: plays/failure.yml

# Step 1b : hook pre run
- name: 'Launch hook pre-run'
  hosts: localhost
  tags:
    - pre-run
    - hook-pre-run
  environment:
    http_proxy: "{{ http_proxy }}"
    https_proxy: "{{ https_proxy }}"
    no_proxy: "{{ no_proxy_list }}"
    PATH: "{{ dci_cluster_configs_dir }}:{{ ansible_env.PATH }}"
  tasks:
    - name: Pre-run hooks
      block:
        - name: Run the pre-run hook
          include_tasks: '{{ hookdir }}/hooks/pre-run.yml'
          when:
            - dci_main == 'install'
          loop: "{{ dci_config_dirs }}"
          loop_control:
            loop_var: hookdir
      rescue: *teardown_error

# Step 1c : Set status to "running"
- name: 'Set running'
  hosts: localhost
  tags:
    - running
  tasks:
    - name: Running
      dci_job:
        id: "{{ hostvars.localhost.job_id }}"
        status: "running"
      delegate_to: localhost
      tags: [dci]

# Step 2 : configure
- name: 'Launch configure'
  hosts: provisioner
  tags:
    - running
    - configure
  environment:
    http_proxy: "{{ http_proxy }}"
    https_proxy: "{{ https_proxy }}"
    no_proxy: "{{ no_proxy_list }}"
    PATH: "{{ dci_cluster_configs_dir }}:{{ ansible_env.PATH }}"
  tasks:
    - name: Configure
      when:
        - (install_type | default('ipi')) != 'acm'
      block:
        # Prepare provisioner host
        - name: Configure provisioner
          include_tasks: plays/configure-provisioner.yml
          when: ansible_distribution_major_version == '8'

        - name: Launch configure
          include_tasks: '{{ hookdir }}/hooks/configure.yml'
          loop: "{{ dci_config_dirs }}"
          loop_control:
            loop_var: hookdir
      rescue: *teardown_error

# Step 3a-1 : installing
- name: 'Launch [IPI/SNO/UPI] install'
  hosts: provisioner
  tags:
    - running
    - installing
  environment:
    http_proxy: "{{ http_proxy }}"
    https_proxy: "{{ https_proxy }}"
    no_proxy: "{{ no_proxy_list }}"
  tasks:
    - name: Install IPI/SNO/UPI
      when:
        - dci_main == 'install'
      block:
        - name: Launch install {{ (install_type | default('ipi'))}}
          include_tasks: "plays/{{ (install_type | default('ipi')) }}-install.yml"
          when:
            - (install_type | default('ipi')) in ['ipi', 'upi', 'sno']

        - name: "Get KUBECONFIG auth/creds from provisioner node"
          fetch:
            src: "{{ item }}"
            dest: "{{ dci_cluster_configs_dir }}/"
            flat: true
          loop:
            - "{{ dir }}/auth/kubeconfig"
            - "{{ dir }}/auth/kubeadmin-password"
          when:
            - (install_type | default('ipi')) in ['ipi', 'upi', 'sno']

      rescue: &teardown_failure
        - name: Run the teardown failure handler
          delegate_to: localhost
          ignore_unreachable: true
          block:
            - name: Failure
              dci_job:
                id: "{{ hostvars.localhost.job_id }}"
                status: "failure"
              delegate_to: localhost
              tags: [dci]

            - name: Run the failure process for partners
              include_tasks: "{{ hookdir }}/hooks/failure.yml"
              loop: "{{ dci_config_dirs }}"
              loop_control:
                loop_var: hookdir
              ignore_errors: true

            - name: Teardown Hooks
              when: dci_teardown_on_failure|bool
              delegate_to: localhost
              ignore_unreachable: true
              ignore_errors: true
              block:
                - name: Run the teardown hooks
                  include_tasks: "{{ hookdir }}/hooks/teardown.yml"
                  loop: "{{ dci_config_dirs | reverse | list }}"
                  loop_control:
                    loop_var: hookdir

                - name: Run the teardown play
                  include_tasks: plays/teardown.yml

          always:
            - name: Run the failure process
              include_tasks: plays/failure.yml

# Step 3a-1 : installing
- name: 'Launch ACM install'
  hosts: localhost
  tags:
    - running
    - installing
    - acm
  environment:
    http_proxy: "{{ http_proxy }}"
    https_proxy: "{{ https_proxy }}"
    no_proxy: "{{ no_proxy_list }}"
    PATH: "{{ dci_cluster_configs_dir }}:{{ ansible_env.PATH }}"
  tasks:
    - name: Install ACM
      block:
        - name: "ACM Installation"
          include_tasks:
            file: plays/acm-install.yml
            apply:
              environment:
                KUBECONFIG: "{{ hub_kubeconfig_path }}"
          when:
            - (install_type | default('ipi')) == 'acm'
            - dci_main == 'install'
      rescue: *teardown_failure

# Step 3a-1 : installing
- name: 'Launch ABI install'
  hosts: localhost
  tags:
    - running
    - installing
    - abi
  environment:
    http_proxy: "{{ http_proxy }}"
    https_proxy: "{{ https_proxy }}"
    no_proxy: "{{ no_proxy_list }}"
    PATH: "{{ dci_cluster_configs_dir }}:{{ ansible_env.PATH }}"
  tasks:
    - name: Install using ABI
      block:
        - name: "ABI Installation"
          include_tasks:
            file: plays/abi-install.yml
          when:
            - (install_type | default('ipi')) == 'abi'
            - dci_main == 'install'
      rescue: *teardown_failure

# Step 3a-1 : installing
- name: 'Launch after install'
  hosts: localhost
  tags:
    - running
    - installing
    - after
  environment:
    http_proxy: "{{ http_proxy }}"
    https_proxy: "{{ https_proxy }}"
    no_proxy: "{{ no_proxy_list }}"
    KUBECONFIG: "{{ kubeconfig_path }}"
    PATH: "{{ dci_cluster_configs_dir }}:{{ ansible_env.PATH }}"
  tasks:
    - name: After Install
      block:
        - name: "Launch after install"
          include_tasks: "plays/after-install.yml"
          when:
            - dci_main == 'install'
      rescue: *teardown_failure

# Step 3a-2 : deploy operators
- name: 'Deploy operators to OCP cluster'
  hosts: localhost
  tags:
    - running
    - operator-deployment
  environment:
    http_proxy: "{{ http_proxy }}"
    https_proxy: "{{ https_proxy }}"
    no_proxy: "{{ no_proxy_list }}"
    KUBECONFIG: "{{ kubeconfig_path }}"
    PATH: "{{ dci_cluster_configs_dir }}:{{ ansible_env.PATH }}"
  tasks:
    - name: Deploy operators
      when:
        - dci_main == 'install'
        - acm_cluster_type | default('') != 'hypershift'
      block:
        - name: "Run cluster health checks"
          include_tasks: plays/check-cluster-health.yml
        - name: "Setup job_logs directory"
          include_tasks: plays/log-dir.yml
          when: job_logs is undefined
        - name: "Deploy operators"
          include_tasks: plays/deploy-operators.yml
      rescue: *teardown_failure

# Step 3a-3 : hook installing
- name: 'Launch hook install'
  hosts: localhost
  tags:
    - running
    - hook-installing
  environment:
    http_proxy: "{{ http_proxy }}"
    https_proxy: "{{ https_proxy }}"
    no_proxy: "{{ no_proxy_list }}"
    KUBECONFIG: "{{ kubeconfig_path }}"
    PATH: "{{ dci_cluster_configs_dir }}:{{ ansible_env.PATH }}"
  tasks:
    - name: Install Hooks
      when:
        - dci_main == 'install'
        - acm_cluster_type | default('') != 'hypershift'
      block:
        - name: "Run cluster health checks"
          include_tasks: plays/check-cluster-health.yml
        - name: "Launch partner install"
          include_tasks: '{{ hookdir }}/hooks/install.yml'
          loop: "{{ dci_config_dirs }}"
          loop_control:
            loop_var: hookdir
      rescue: *teardown_failure

# Step 3b-1 : upgrading
- name: 'Launch upgrade'
  hosts: localhost
  tags:
    - running
    - upgrading
  environment:
    http_proxy: "{{ http_proxy }}"
    https_proxy: "{{ https_proxy }}"
    no_proxy: "{{ no_proxy_list }}"
    KUBECONFIG: "{{ kubeconfig_path }}"
    PATH: "{{ dci_cluster_configs_dir }}:{{ ansible_env.PATH }}"
  tasks:
    - name: Upgrade
      when:
        - dci_main == 'upgrade'
      block:
        - name: "Run cluster health checks"
          include_tasks: plays/check-cluster-health.yml
        - name: "Launch upgrade"
          include_tasks: plays/upgrade.yml
      rescue: *teardown_failure

# Step 3b-2 : hook upgrading
- name: 'Launch hook upgrade'
  hosts: localhost
  tags:
    - running
    - hook-upgrading
  environment:
    http_proxy: "{{ http_proxy }}"
    https_proxy: "{{ https_proxy }}"
    no_proxy: "{{ no_proxy_list }}"
    KUBECONFIG: "{{ kubeconfig_path }}"
    PATH: "{{ dci_cluster_configs_dir }}:{{ ansible_env.PATH }}"
  tasks:
    - name: Upgrade Hooks
      when:
        - dci_main == 'upgrade'
      block:
        - name: "Run cluster health checks"
          include_tasks: plays/check-cluster-health.yml
        - name: "Launch partner upgrade"
          include_tasks: '{{ hookdir }}/hooks/upgrade.yml'
          loop: "{{ dci_config_dirs }}"
          loop_control:
            loop_var: hookdir
      rescue: *teardown_failure

# Step 4 : Red Hat testing
- name: 'Launch Red Hat tests'
  hosts: localhost
  tags:
    - running
    - testing
    - redhat-testing
  environment:
    http_proxy: "{{ http_proxy }}"
    https_proxy: "{{ https_proxy }}"
    no_proxy: "{{ no_proxy_list }}"
    KUBECONFIG: "{{ kubeconfig_path }}"
    PATH: "{{ dci_cluster_configs_dir }}:{{ ansible_env.PATH }}"
  tasks:
    - name: Red Hat tests
      when:
        - (install_type | default('ipi')) != 'acm'
      block:
        - name: "Run cluster health checks"
          include_tasks: plays/check-cluster-health.yml
        - name: "Setup job_logs directory"
          include_tasks: plays/log-dir.yml
          when: job_logs is undefined
        - name: "Launch Red Hat tests"
          include_tasks: plays/tests.yml
      rescue: *teardown_failure

# Step 5 : partner testing
- name: 'Launch partner tests'
  hosts: localhost
  tags:
    - running
    - testing
    - partner-testing
  environment:
    http_proxy: "{{ http_proxy }}"
    https_proxy: "{{ https_proxy }}"
    no_proxy: "{{ no_proxy_list }}"
    KUBECONFIG: "{{ kubeconfig_path }}"
    PATH: "{{ dci_cluster_configs_dir }}:{{ ansible_env.PATH }}"
  tasks:
    - name: Test Hooks
      when:
        - acm_cluster_type | default('') != 'hypershift'
      block:
        - name: "Run cluster health checks"
          include_tasks: plays/check-cluster-health.yml
        - name: "Setup job_logs directory"
          include_tasks: plays/log-dir.yml
          when: job_logs is undefined
        - name: Run the partner tests
          include_tasks: '{{ hookdir }}/hooks/tests.yml'
          loop: "{{ dci_config_dirs }}"
          loop_control:
            loop_var: hookdir
        - name: "Run cluster health checks"
          include_tasks: plays/check-cluster-health.yml
      rescue: *teardown_failure

    - name: Post-run
      dci_job:
        id: "{{ job_id }}"
        status: "post-run"
      tags: [dci]

# Step 6 : post run
- name: Launch post run
  hosts: localhost
  tags:
    - post-run
    - dci
  environment:
    http_proxy: "{{ http_proxy }}"
    https_proxy: "{{ https_proxy }}"
    no_proxy: "{{ no_proxy_list }}"
    KUBECONFIG: "{{ kubeconfig_path }}"
    PATH: "{{ dci_cluster_configs_dir }}:{{ ansible_env.PATH }}"
  tasks:
    - name: Post-run
      block:
        - name: Run the partner post-run
          include_tasks: '{{ hookdir }}/hooks/post-run.yml'
          loop: "{{ dci_config_dirs }}"
          loop_control:
            loop_var: hookdir

        - name: Run the post-run
          include_tasks: plays/post-run.yml

      rescue: *teardown_error

    - name: Success
      dci_job:
        id: "{{ job_id }}"
        status: "success"
      tags: [dci]

# Step 7 : state "success"
- name: 'Success'
  hosts: localhost
  tags:
    - success
  environment:
    http_proxy: "{{ http_proxy }}"
    https_proxy: "{{ https_proxy }}"
    no_proxy: "{{ no_proxy_list }}"
    KUBECONFIG: "{{ kubeconfig_path }}"
    PATH: "{{ dci_cluster_configs_dir }}:{{ ansible_env.PATH }}"
  tasks:
    - name: Success
      ignore_unreachable: true
      ignore_errors: true
      block:
        - name: Run the success hook
          include_tasks: '{{ hookdir }}/hooks/success.yml'
          loop: "{{ dci_config_dirs }}"
          loop_control:
            loop_var: hookdir

        - name: Run the success
          include_tasks: plays/success.yml

        - name: Teardown on success
          when: dci_teardown_on_success | bool
          block:
            - name: Run the teardown hooks
              include_tasks: "{{ hookdir }}/hooks/teardown.yml"
              loop: "{{ dci_config_dirs | reverse | list }}"
              loop_control:
                loop_var: hookdir

            - name: Run the teardown play
              include_tasks: plays/teardown.yml

...
