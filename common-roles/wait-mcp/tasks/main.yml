---

- name: Wait for updated machine configs to be applied on the nodes
  block:
    - name: "Pause for Machine Config to be created"
      pause:
        seconds: 60

    - name: Get Machine config pools status
      k8s_info:
        api_version: machineconfiguration.openshift.io/v1
        kind: MachineConfigPool
        kubeconfig: "{{ kubeconfig_path }}"
      register: reg_mcpool_status
      vars:
        status_query: "resources[*].status.conditions[?type=='Updated'].status"
        update_status: "{{ reg_mcpool_status | json_query(status_query) | flatten | unique }}"
      until:
        - update_status == ['True']
      retries: "{{ mcp_wait_retries }}"
      delay: "{{ mcp_wait_delay }}"
      delegate_to: localhost
  rescue:
    - name: Wait for updated machine configs to be applied on the nodes failed - no workaround applied
      fail:
        msg: "No workaround is defined, so the job has to fail at this point"
      when:
        - '"bz2062038" not in dci_workarounds|default([])'
        - '"bz2050394" not in dci_workarounds|default([])'

    # Bugzilla report: https://bugzilla.redhat.com/show_bug.cgi?id=2053445
    # Sometimes after applying MCPs some nodes remain unschedulable.
    - name: Workaround for BZ-2053445 - force workers uncordon
      block:
        - name: Check for workers with Scheduling disabled
          environment:
            KUBECONFIG: "{{ kubeconfig_path }}"
          shell: >
            {{ oc_tool_path }} get nodes --no-headers=true |
            grep 'Ready,SchedulingDisabled' | grep worker | awk '{ print $1 }'
          register: reg_disabled_nodes
          delegate_to: localhost

        # The workaround should not be applied if we're dealing with a network problem here.
        # To verify that network is doing fine, we check that all ovnkube-node-XXXX pods are running.
        - name: Check the state of ovnkube-node-XXX pods
          environment:
            KUBECONFIG: "{{ kubeconfig_path }}"
          shell: >
            {{ oc_tool_path }} get pods -n openshift-ovn-kubernetes --no-headers=true |
            awk '! /Running/ && /ovnkube-node/'
          register: bad_ovnkube_pods
          delegate_to: localhost

        - name: Fail when network is in a bad state
          fail:
            msg: "Failing because some of ovnkube-node-XXX pods are not in a running state"
          when: bad_ovnkube_pods.stdout | length > 0

        - name: Uncordon workers if all pods ovnkube-node-XXX are Running
          block:
            - name: Workaround - uncordon all ready workers with disabled scheduling
              environment:
                KUBECONFIG: "{{ kubeconfig_path }}"
              shell: >
                {{ oc_tool_path }} adm uncordon {{ item }}
              loop: "{{ reg_disabled_nodes.stdout.split('\n') }}"
              delegate_to: localhost

            - name: Wait for workers to be ready
              environment:
                KUBECONFIG: "{{ kubeconfig_path }}"
              shell: >
                {{ oc_tool_path }} get nodes --no-headers=true | grep worker
              register: nodes
              until:
                - '"SchedulingDisabled" not in nodes.stdout'
              retries: 30
              delay: 10
              delegate_to: localhost
          when:
            - reg_disabled_nodes.stdout | length > 0
            - bad_ovnkube_pods.stdout | length == 0
      when:
        - dci_workarounds is defined
        - '"bz2062038" in dci_workarounds' or '"bz2050394" in dci_workarounds'

...
