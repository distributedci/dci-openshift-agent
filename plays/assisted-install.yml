---
- name: Copy facts from localhost to bastion
  hosts: bastion
  tasks:
    - name: Set facts
      set_fact:
        pull_secret: "{{ hostvars['localhost']['pull_secret'] }}"

    - name: Set registry_repository fact for install playbooks in disconnected mode
      set_fact:
        registry_repository: "{{ hostvars[groups['registry_host'][0]].local_repo | default( 'ocp-'+ hostvars['localhost']['version'].split('.')[:2] | join('.') +'/'+ hostvars['localhost']['version'], true) }}"
      when: dci_disconnected | default(False) | bool

- import_playbook: crucible/deploy_cluster_agent_based_installer.yml
  when: (use_agent_based_installer | default(true)) | bool

- import_playbook: crucible/deploy_cluster_assisted_installer.yml
  when: not ((use_agent_based_installer | default(true)) | bool)

- name: Reset vars for DCI
  hosts: bastion
  vars_files:
    - ../group_vars/all
  tasks:
    - name: Reset cluster var for DCI
      set_fact:
        cluster: "{{ cluster_name }}"

- name: Get kubeconfig
  hosts: localhost
  vars:
    secure: false
    ASSISTED_INSTALLER_BASE_URL: "{{ secure | ternary('https', 'http') }}://{{ hostvars['assisted_installer']['host'] }}:{{ hostvars['assisted_installer']['port'] }}/api/assisted-install/v2"
    CLUSTER_ID: "{{ cluster_id | default(hostvars['bastion']['cluster_id']) }}"
    URL_ASSISTED_INSTALLER_CLUSTER: "{{ ASSISTED_INSTALLER_BASE_URL }}/clusters/{{ CLUSTER_ID }}"
    kube_filename: "kubeconfig"
    dest_dir: "{{ kubeconfig_dest_dir | default(ansible_env.HOME) }}"
    kubeconfig_path: "{{ dest_dir }}/{{ kube_filename }}"
    kubeadmin_vault_name: "kubeadmin-password"
  vars_files:
    - ../group_vars/all
  environment:
    KUBECONFIG: "{{ kubeconfig_path }}"
    PATH: "{{ dci_cluster_configs_dir }}:{{ ansible_env.PATH }}"
  tasks:
    - block:
      - name: Download kubeconfig (onprem)
        get_url:
          url: "{{ URL_ASSISTED_INSTALLER_CLUSTER }}/downloads/credentials?file_name=kubeconfig"
          dest: "{{ kubeconfig_path }}"
          mode: 0664
        when: not (use_agent_based_installer | default(true)) | bool

      - name: Copy generated kubeconfig (ABI)
        copy:
          src: "{{ repo_root_path }}/generated/{{ cluster_name }}/auth/kubeconfig"
          dest: "{{ kubeconfig_path }}"
        when: (use_agent_based_installer | default(true)) | bool

      - name: Perform simple connectivity check with oc
        shell: |
          {{ oc_tool_path }} explain pods
        register: result
        retries: 3
        delay: 10
        until: result.rc == 0

      - name: Check status of cluster operators
        block:
          - name: Wait up to 20 mins for cluster to become functional
            shell: |
              {{ oc_tool_path }} wait clusteroperators --all --for=condition=Available --timeout=20m
            register: result
            retries: 3
            delay: 10
            until: result.rc == 0
        rescue:
          - name: Get better info for failure message
            shell: oc get clusteroperators
            register: co_result

          - name: Fail Cluster
            fail:
              msg: |
                Cluster has not come up correctly:
                  {{ co_result.stdout }}

      - name: Store admin credentials (onprem)
        when: not (use_agent_based_installer | default(true)) | bool
        block:
          - name: Get credentials
            uri:
              url: "{{ URL_ASSISTED_INSTALLER_CLUSTER }}/credentials"
              return_content: yes
            register: credentials

          - name: Login to add token to kubeconfig
            shell: |
              {{ oc_tool_path }} login -u {{ credentials.json.username }} -p '{{ credentials.json.password }}'

          - name: Save credentials to file
            copy:
              content: "{{ credentials.json | to_yaml }}"
              dest: "{{ dest_dir }}/{{ kubeadmin_vault_name }}"
              mode: 0600

          - name: Save credentials to vault
            shell:
              cmd: "ansible-vault encrypt --vault-password-file {{ kubeadmin_vault_password_file_path }} {{ dest_dir }}/{{ kubeadmin_vault_name }}"
            when: (kubeadmin_vault_password_file_path is defined) and (kubeadmin_vault_password_file_path is file)

      - name: Store admin credentials (ABI)
        when: (use_agent_based_installer | default(true)) | bool
        block:
          - name: Read credentials
            slurp:
              src: "{{ repo_root_path }}/generated/{{ cluster_name }}/auth/kubeadmin-password"
            register: kubeadmin_password

          - name: Set credentials map
            set_fact:
              kubeadmin_credentials:
                username: kubeadmin
                password: "{{ kubeadmin_password.content | b64decode }}"
                console_url: "https://console-openshift-console.apps.{{ cluster_name }}.{{ base_dns_domain }}"

          - name: Save credentials to file
            copy:
              content: "{{ kubeadmin_credentials | to_yaml }}"
              dest: "{{ dest_dir }}/{{ kubeadmin_vault_name }}"
              mode: 0600

          - name: Save credentials to vault
            shell:
              cmd: "ansible-vault encrypt --vault-password-file {{ kubeadmin_vault_password_file_path }} {{ dest_dir }}/{{ kubeadmin_vault_name }}"
            when: (kubeadmin_vault_password_file_path is defined) and (kubeadmin_vault_password_file_path is file)

      - name: Set kubeconfig_path for provisioner
        set_fact:
          kubeconfig_path: "{{ kubeconfig_path }}"
        delegate_to: "{{ groups['provisioner'] | first }}"
        delegate_facts: true

      - name: Check if all cluster-operators are running correctly
        community.kubernetes.k8s_info:
          kind: ClusterOperator
        register: clusteroperator_info
        vars:
          status_query: "resources[*].status.conditions[?type=='Available'].status"
          cluster_operators_available: "{{ clusteroperator_info | json_query(status_query) | flatten | unique }}"
        retries: 6
        delay: 10
        until: cluster_operators_available == ['True']

      - name: Check if ClusterVersion is Complete
        community.kubernetes.k8s_info:
          api: config.openshift.io/v1
          kind: ClusterVersion
          name: version
        register: cluster_version
        vars:
          status_query: "resources[*].status.history[?state=='Completed'].state"
          cluster_version_available: "{{ cluster_version | json_query(status_query) | flatten | unique }}"
        retries: 60
        delay: 10
        until: cluster_version_available == ['Completed']

      rescue:
        - name: Run the teardown failure handler
          block:
            - name: failure
              dci_job:
                id: "{{ hostvars.localhost.job_id }}"
                status: "failure"
              delegate_to: localhost
              tags: [dci]

            - name: Run the failure process for partners
              include_tasks: "{{ hookdir }}/hooks/failure.yml"
              loop: "{{ dci_config_dirs }}"
              loop_control:
                loop_var: hookdir
              ignore_errors: true

            - block:
                - name: Run the teardown hooks
                  include_tasks: "{{ hookdir }}/hooks/teardown.yml"
                  loop: "{{ dci_config_dirs }}"
                  loop_control:
                    loop_var: hookdir

                - name: Run the teardown play
                  include_tasks: teardown.yml
              when: dci_teardown_on_failure|bool
              delegate_to: localhost
              ignore_unreachable: true
              ignore_errors: true
          always:
            - name: Run the failure process
              include_tasks: failure.yml
          delegate_to: localhost
          ignore_unreachable: true

- name: Assisted installer Cleanup
  hosts: localhost
  vars_files:
    - ../group_vars/all
  tasks:
    - name: Run Assisted installer cleanup
      vars:
        repo_root_path: "{{ dci_cluster_configs_dir }}"
      include_role:
        name: redhatci.ocp.populate_mirror_registry
        tasks_from: cleanup.yml
...
