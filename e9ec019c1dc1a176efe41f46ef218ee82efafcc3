{
  "comments": [
    {
      "unresolved": false,
      "key": {
        "uuid": "9d727ec9_6081b9a9",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 17
      },
      "lineNbr": 0,
      "author": {
        "id": 938
      },
      "writtenOn": "2023-08-29T09:32:39Z",
      "side": 1,
      "message": "check dallas ocp-4.14-vanilla",
      "revId": "e9ec019c1dc1a176efe41f46ef218ee82efafcc3",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "2bbe1238_79193af4",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 17
      },
      "lineNbr": 0,
      "author": {
        "id": 938
      },
      "writtenOn": "2023-08-29T12:54:00Z",
      "side": 1,
      "message": "Hello everyone, the change is tested:\n\n1/ Libvirt job with green operator status: \nhttps://www.distributed-ci.io/jobs/4347b257-78b0-473b-a33a-b53c9e6fb2a3/jobStates?sort\u003ddate\u0026task\u003d5c3cd993-8898-4c4d-aca3-54c8084381e7\n\n\n2/ Dallas job with degraded operators:\nhttps://www.distributed-ci.io/jobs/02534e29-bc32-4ae4-af24-b907ba1806cf/jobStates?sort\u003ddate\u0026task\u003d4989e3a5-9cf8-4ff6-ac06-2a0f2f47be62\n\nThe output of the check I added: https://www.distributed-ci.io/jobs/02534e29-bc32-4ae4-af24-b907ba1806cf/jobStates?sort\u003ddate\u0026task\u003df90d7be7-42fa-42e0-923e-98bc4b46975a\n\n{\"degraded_operators\": [\n[{\"lastTransitionTime\": \"2023-08-29T11:56:32Z\", \"message\": \"OAuthServerRouteEndpointAccessibleControllerDegraded: Get \\\"https://oauth-openshift.apps.cluster3.dfwt5g.lab/healthz\\\": dial tcp: lookup oauth-openshift.apps.cluster3.dfwt5g.lab on 172.30.0.10:53: read udp 10.129.0.21:38956-\u003e172.30.0.10:53: read: connection refused\", \"reason\": \"OAuthServerRouteEndpointAccessibleController_SyncError\", \"status\": \"True\", \"type\": \"Degraded\"}, \n\"authentication\"], \n[{\"lastTransitionTime\": \"2023-08-29T11:59:47Z\", \"message\": \"The \\\"default\\\" ingress controller reports Degraded\u003dTrue: DegradedConditions: One or more other status conditions indicate a degraded state: CanaryChecksSucceeding\u003dFalse (CanaryChecksRepetitiveFailures: Canary route checks for the default ingress controller are failing)\", \"reason\": \"IngressDegraded\", \"status\": \"True\", \"type\": \"Degraded\"}, \n\"ingress\"], \n[{\"lastTransitionTime\": \"2023-08-29T12:01:15Z\", \"message\": \"GarbageCollectorDegraded: error querying alerts: Post \\\"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query\\\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: read udp 10.129.0.37:34226-\u003e172.30.0.10:53: read: connection refused\", \"reason\": \"GarbageCollector_Error\", \"status\": \"True\", \"type\": \"Degraded\"}, \n\"kube-controller-manager\"]]}\n\n\nLet\u0027s check the cluster info to ensure that everything\u0027s working as expected:\nhttps://www.distributed-ci.io/jobs/02534e29-bc32-4ae4-af24-b907ba1806cf/jobStates?sort\u003ddate\u0026task\u003d4989e3a5-9cf8-4ff6-ac06-2a0f2f47be62\n\ndns \u003d the last timestamp has status\u003dTrue type\u003dProgressing --\u003e it\u0027s NOT reported as degraded (even if 30 min ago it had status\u003dTrue type\u003dDegraded)\n\"conditions\": [\n{\"lastTransitionTime\": \"2023-08-29T11:54:46Z\", \"message\": \"DNS \\\"default\\\" is unavailable.\", \"reason\": \"DNSUnavailable\", \"status\": \"False\", \"type\": \"Available\"}, \n{\"lastTransitionTime\": \"2023-08-29T12:02:00Z\", \"message\": \"DNS \\\"default\\\" reports Progressing\u003dTrue: \\\"Have 0 available DNS pods, want 7.\\\"\", \"reason\": \"DNSReportsProgressingIsTrue\", \"status\": \"True\", \"type\": \"Progressing\"}, {\"lastTransitionTime\": \"2023-08-29T11:32:21Z\", \"message\": \"DNS default is degraded\", \"reason\": \"DNSDegraded\", \"status\": \"True\", \"type\": \"Degraded\"}, \n{\"lastTransitionTime\": \"2023-08-29T10:31:59Z\", \"message\": \"DNS default is upgradeable: DNS Operator can be upgraded\", \"reason\": \"DNSUpgradeable\", \"status\": \"True\", \"type\": \"Upgradeable\"}]\n\n\nauthentication \u003d the last timestamp has status\u003dTrue type\u003dDegraded --\u003e it\u0027s reported as degraded\n\"conditions\": [\n{\"lastTransitionTime\": \"2023-08-29T11:56:32Z\", \"message\": \"OAuthServerRouteEndpointAccessibleControllerDegraded: Get \\\"https://oauth-openshift.apps.cluster3.dfwt5g.lab/healthz\\\": dial tcp: lookup oauth-openshift.apps.cluster3.dfwt5g.lab on 172.30.0.10:53: read udp 10.129.0.21:38956-\u003e172.30.0.10:53: read: connection refused\", \"reason\": \"OAuthServerRouteEndpointAccessibleController_SyncError\", \"status\": \"True\", \"type\": \"Degraded\"}, \n{\"lastTransitionTime\": \"2023-08-29T11:33:14Z\", \"message\": \"AuthenticatorCertKeyProgressing: All is well\", \"reason\": \"AsExpected\", \"status\": \"False\", \"type\": \"Progressing\"}, \n{\"lastTransitionTime\": \"2023-08-29T11:54:32Z\", \"message\": \"OAuthServerRouteEndpointAccessibleControllerAvailable: Get \\\"https://oauth-openshift.apps.cluster3.dfwt5g.lab/healthz\\\": dial tcp: lookup oauth-openshift.apps.cluster3.dfwt5g.lab on 172.30.0.10:53: read udp 10.129.0.21:38956-\u003e172.30.0.10:53: read: connection refused\", \"reason\": \"OAuthServerRouteEndpointAccessibleController_EndpointUnavailable\", \"status\": \"False\", \"type\": \"Available\"}, \n{\"lastTransitionTime\": \"2023-08-29T10:30:47Z\", \"message\": \"All is well\", \"reason\": \"AsExpected\", \"status\": \"True\", \"type\": \"Upgradeable\"}]\n\n\ningress \u003d the last timestamp has status\u003dTrue type\u003dDegraded --\u003e it\u0027s reported as degraded\n\"conditions\": [\n{\"lastTransitionTime\": \"2023-08-29T11:06:31Z\", \"message\": \"The \\\"default\\\" ingress controller reports Available\u003dTrue.\", \"reason\": \"IngressAvailable\", \"status\": \"True\", \"type\": \"Available\"}, \n{\"lastTransitionTime\": \"2023-08-29T11:53:12Z\", \"message\": \"desired and current number of IngressControllers are equal\", \"reason\": \"AsExpected\", \"status\": \"False\", \"type\": \"Progressing\"}, \n{\"lastTransitionTime\": \"2023-08-29T11:59:47Z\", \"message\": \"The \\\"default\\\" ingress controller reports Degraded\u003dTrue: DegradedConditions: One or more other status conditions indicate a degraded state: CanaryChecksSucceeding\u003dFalse (CanaryChecksRepetitiveFailures: Canary route checks for the default ingress controller are failing)\", \"reason\": \"IngressDegraded\", \"status\": \"True\", \"type\": \"Degraded\"}, \n{\"lastTransitionTime\": \"2023-08-29T10:31:58Z\", \"reason\": \"IngressControllersUpgradeable\", \"status\": \"True\", \"type\": \"Upgradeable\"}, \n{\"lastTransitionTime\": \"2023-08-29T10:31:58Z\", \"reason\": \"AsExpected\", \"status\": \"False\", \"type\": \"EvaluationConditionsDetected\"}]\n\n\nkube-controller-manager \u003d the last timestamp has status\u003dTrue type\u003dDegraded --\u003e it\u0027s reported as degraded\n\"conditions\": [\n{\"lastTransitionTime\": \"2023-08-29T12:01:15Z\", \"message\": \"GarbageCollectorDegraded: error querying alerts: Post \\\"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query\\\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: read udp 10.129.0.37:34226-\u003e172.30.0.10:53: read: connection refused\", \"reason\": \"GarbageCollector_Error\", \"status\": \"True\", \"type\": \"Degraded\"}, \n{\"lastTransitionTime\": \"2023-08-29T10:44:14Z\", \"message\": \"NodeInstallerProgressing: 3 nodes are at revision 7\", \"reason\": \"AsExpected\", \"status\": \"False\", \"type\": \"Progressing\"}, \n{\"lastTransitionTime\": \"2023-08-29T10:34:25Z\", \"message\": \"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 7\", \"reason\": \"AsExpected\", \"status\": \"True\", \"type\": \"Available\"}, \n{\"lastTransitionTime\": \"2023-08-29T10:30:54Z\", \"message\": \"All is well\", \"reason\": \"AsExpected\", \"status\": \"True\", \"type\": \"Upgradeable\"}]\n\nThanks for reading up until here :) ",
      "revId": "e9ec019c1dc1a176efe41f46ef218ee82efafcc3",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "a22d0000_e04d5724",
        "filename": "plays/check-cluster-health.yml",
        "patchSetId": 17
      },
      "lineNbr": 11,
      "author": {
        "id": 648
      },
      "writtenOn": "2023-08-29T18:47:37Z",
      "side": 1,
      "message": "you can tackle this in a different way:\n\n1) don\u0027t add a pause at the beginning\n2) change your single task to a polling task similar to the one you\u0027re replacing but sorted by timestamp. You could optionally fail_when zipping returns a length of 1 with Degraded/True\n3) have an ansible.builtin.fail task after that would check if the last status is healthy, this would catch when zipping returns length \u003e 1, hopefully it went Degraded/True -\u003e Degraded/True -\u003e Degraded/False but it will also catch when it goes Degraded/True -\u003e Degraded/False -\u003e Degraded/True",
      "revId": "e9ec019c1dc1a176efe41f46ef218ee82efafcc3",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "56144da6_07deda49",
        "filename": "plays/check-cluster-health.yml",
        "patchSetId": 17
      },
      "lineNbr": 11,
      "author": {
        "id": 938
      },
      "writtenOn": "2023-08-29T19:36:27Z",
      "side": 1,
      "message": "Hi Jorge, yes, I was thinking of doing this at the very beginning\n\n  name: \"Fail if there are degraded cluster-operators - {{ version }}\"\n  community.kubernetes.k8s_info:\n    kind: ClusterOperator\n  register: clusteroperator_info\n  vars:\n    operator_conditions: \"{{ clusteroperator_info.resources | map(attribute\u003d\u0027status.conditions\u0027) }}\"\n    operator_last_conditions: \"{{ operator_conditions | map(\u0027sort\u0027, attribute\u003d\u0027lastTransitionTime\u0027) | map(\u0027last\u0027) | list }}\"\n    degraded_conditions: \"{{ operator_last_conditions | selectattr(\u0027type\u0027, \u0027eq\u0027, \u0027Degraded\u0027) | selectattr(\u0027status\u0027, \u0027eq\u0027, \u0027True\u0027) | list }}\"\n  failed_when: degraded_conditions | length \u003e 0\n  retries: 3\n  delay: 10\n  \nThe issue is that this way debugging becomes less convenient when there are degraded operators: we would have no clusteroperator_info and no information about which operators degraded and why.\n\nMy current change consists of 2 tasks instead of one, but we have a zipped list of degraded operators and why they degraded. We also have complete clusteroperator_info if we need to debug further.\n\nWhat do you think?",
      "parentUuid": "a22d0000_e04d5724",
      "revId": "e9ec019c1dc1a176efe41f46ef218ee82efafcc3",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "85d340fd_290216ea",
        "filename": "plays/check-cluster-health.yml",
        "patchSetId": 17
      },
      "lineNbr": 11,
      "author": {
        "id": 648
      },
      "writtenOn": "2023-08-29T23:22:24Z",
      "side": 1,
      "message": "I see what you mean, I came up with the following:\n```\n    - name: Wait for ClusterOperator to settle\n      ansible.builtin.pause:\n        seconds: \"{{ clusteroperator_settle_seconds | default(30) }}\"\n\n    - name: Fetch ClusterOperator info\n      community.kubernetes.k8s_info:\n        kind: ClusterOperator\n      register: operator_info\n\n    - name: Cleanup into a more manageable structucture\n      ansible.builtin.set_fact:\n        operator_status: \"{{ operator_info | to_json | from_json | json_query(\u0027resources[*].{name: metadata.name, last: sort_by(status.conditions, \u0026lastTransitionTime)[-1]}\u0027) }}\"  # \nfrom/to json silliness thanks to a jinja2 bug\n\n    - name: Debug for troubleshooting\n      ansible.builtin.debug:\n        var: operator_status\n\n    - name: Fail if any ClusterOperator last status is Degraded\n      ansible.builtin.fail:\n        msg: \"There are operators in Degraded state\"                                      \n      when: \u003e-\n        operator_status |\n        selectattr(\u0027last.type\u0027, \u0027eq\u0027, \u0027Degraded\u0027) |                                       \n        selectattr(\u0027last.status\u0027, \u0027eq\u0027, \u0027True\u0027) |                                         \n        list |\n        length \u003e 0\n```\n\nwdyt?",
      "parentUuid": "56144da6_07deda49",
      "revId": "e9ec019c1dc1a176efe41f46ef218ee82efafcc3",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "7862dbd8_60f5e03e",
        "filename": "plays/check-cluster-health.yml",
        "patchSetId": 17
      },
      "lineNbr": 11,
      "author": {
        "id": 938
      },
      "writtenOn": "2023-08-30T07:46:56Z",
      "side": 1,
      "message": "Thanks, Jorge. Doing a sort within the json_query is exactly what I was searching for. I initially thought it was impossible. Thank you for demonstrating how to do it, I\u0027m adopting this idea. \n\nI\u0027ve tested your query locally for both degraded and green scenarios, and it works perfectly. Running Libvirt/Dallas tests now.",
      "parentUuid": "85d340fd_290216ea",
      "revId": "e9ec019c1dc1a176efe41f46ef218ee82efafcc3",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "c2ca4960_4b0e0e92",
        "filename": "plays/check-cluster-health.yml",
        "patchSetId": 17
      },
      "lineNbr": 11,
      "author": {
        "id": 648
      },
      "writtenOn": "2023-08-30T21:36:25Z",
      "side": 1,
      "message": "If you tried to use sort_by before and you got a weird error you may have hit this bug https://github.com/ansible-collections/community.general/issues/320 that\u0027s why I added `to_json | from_json` as it was listed as a workaround in that bug, silly I know, but seems to work.",
      "parentUuid": "7862dbd8_60f5e03e",
      "revId": "e9ec019c1dc1a176efe41f46ef218ee82efafcc3",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    }
  ]
}