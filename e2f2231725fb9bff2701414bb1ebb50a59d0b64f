{
  "comments": [
    {
      "unresolved": false,
      "key": {
        "uuid": "2647e78a_623dd806",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 667
      },
      "writtenOn": "2022-06-24T17:19:35Z",
      "side": 1,
      "message": "I saw the other changes and I\u0027m not sure what\u0027s the gain on this change, I see the code for SRIOV is few lines and I see in here the code growth considerably.  Is truly an advantage on this change? As to me this code is becoming more complex now than it used to be. Thanks!",
      "revId": "e2f2231725fb9bff2701414bb1ebb50a59d0b64f",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "a507d2db_068d04f2",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 800
      },
      "writtenOn": "2022-06-24T17:34:01Z",
      "side": 1,
      "message": "Hi Tony, thanks for your comment. We can review this next Monday, but mainly, the two objectives of this change are:\n\n- First of all, to use the same role to do the MCP check (done on d-o-a, d-o-a-a and example-cnf, mainly) and the SRIOV check (done on SPK and BigIP hooks), as the workaround is mostly the same but the only change is the resource to be checked\n- Secondly, to add retries to this role, which is currently done on check-sriov but not on wait-mcp, and it had happened that the workaround was applied, the check passed, but a new node moved to SchedulingDisabled after a while and the job failed in a further step of the job. With this, we managed to avoid this effect on BigIP/SPK.",
      "parentUuid": "2647e78a_623dd806",
      "revId": "e2f2231725fb9bff2701414bb1ebb50a59d0b64f",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "be87f93e_edc7fefd",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 938
      },
      "writtenOn": "2022-06-25T12:41:43Z",
      "side": 1,
      "message": "Hi Ramon, I see that you\u0027re trying to add some sort of abstraction with resource_to_check, to treat SriovNetworkNodeState and MachineConfigPool in the same way. This abstraction doesn\u0027t seem to work - you have \"if SriovNetworkNodeState\" / \"if MachineConfigPool\" everywhere. Besides, that makes the code to be twice long than before and really confusing.\n\nWouldn\u0027t it be better to split the code into three different plays within the common role? Something like this:\n\ncommon-roles:\n- mcp_workaround.yml\n- sriov_workaround.yml\n- uncordon_workers.yml   \u003c-- to move the common part into the play apart and call it from mcp_workaround and sriov_workaround?",
      "revId": "e2f2231725fb9bff2701414bb1ebb50a59d0b64f",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "3398b219_8d69e3d2",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 800
      },
      "writtenOn": "2022-06-27T11:31:07Z",
      "side": 1,
      "message": "Hi Tatiana, yes, it could be a good improvement. The thing is, I have to see how to do this, because in this change we are doing retries of the whole role, and if I add a play that is used by both mcp and sriov workarounds... I have to figure out how to optimize this. I\u0027ll take a look and propose a new patchset soon.\n\nAnyway, job is not failing because of the definition done. In fact, it passed the check-resource: https://www.distributed-ci.io/jobs/a93a219e-0532-4de1-8824-0ef0871e3b09/jobStates#18d8cfe3-f6a4-4048-95b0-377fe53ac740:file326",
      "parentUuid": "be87f93e_edc7fefd",
      "revId": "e2f2231725fb9bff2701414bb1ebb50a59d0b64f",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "978ffe58_29e1b858",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 673
      },
      "writtenOn": "2022-06-27T13:30:04Z",
      "side": 1,
      "message": "I have few questions on the delegate_host usage",
      "revId": "e2f2231725fb9bff2701414bb1ebb50a59d0b64f",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "98c0d529_9a1e6755",
        "filename": "common-roles/check-resource/tasks/main.yml",
        "patchSetId": 1
      },
      "lineNbr": 7,
      "author": {
        "id": 673
      },
      "writtenOn": "2022-06-27T13:30:04Z",
      "side": 1,
      "message": "This \u0027Pause task\u0027 looks like  a dirty patch in a role like this. Is it really necessary? The following check is covering it right?",
      "revId": "e2f2231725fb9bff2701414bb1ebb50a59d0b64f",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "411d0410_360f90b0",
        "filename": "common-roles/check-resource/tasks/main.yml",
        "patchSetId": 1
      },
      "lineNbr": 7,
      "author": {
        "id": 800
      },
      "writtenOn": "2022-06-27T14:00:54Z",
      "side": 1,
      "message": "I\u0027ve removed this in the new patchset, this pause was inherited from wait-mcp, buy maybe we can omit it, yes. Let\u0027s see how it goes.",
      "parentUuid": "98c0d529_9a1e6755",
      "revId": "e2f2231725fb9bff2701414bb1ebb50a59d0b64f",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "05bbe5f6_fbb4fd36",
        "filename": "common-roles/check-resource/tasks/main.yml",
        "patchSetId": 1
      },
      "lineNbr": 75,
      "author": {
        "id": 673
      },
      "writtenOn": "2022-06-27T13:30:04Z",
      "side": 1,
      "message": "Why do we need to run this on the jumphost and why this could cause problems?",
      "revId": "e2f2231725fb9bff2701414bb1ebb50a59d0b64f",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "8b1ad6e5_1a966fdc",
        "filename": "common-roles/check-resource/tasks/main.yml",
        "patchSetId": 1
      },
      "lineNbr": 75,
      "author": {
        "id": 800
      },
      "writtenOn": "2022-06-27T14:00:54Z",
      "side": 1,
      "message": "The \"may cause problems\" comment, honestly, I don\u0027t know why. This, again, is inherited from wait-mcp and I\u0027ve removed that comment.\n\nThe reason of executing this on the jumphost is because wait-mcp has been called, up to now, from different stages in d-o-a and d-o-a-a, some of them running in the jumphost, but others in the provisionhost. So, to harmonize all the calls, we are forcing to run all this in the jumphost, to use the same reference for the kubeconfig, oc_tool_path and similar variables, in order to avoid problems with this. That\u0027s the main reason, and I\u0027ve followed that approach in this refactor too.",
      "parentUuid": "05bbe5f6_fbb4fd36",
      "revId": "e2f2231725fb9bff2701414bb1ebb50a59d0b64f",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "6a202929_527ce196",
        "filename": "common-roles/check-resource/tasks/main.yml",
        "patchSetId": 1
      },
      "lineNbr": 91,
      "author": {
        "id": 673
      },
      "writtenOn": "2022-06-27T13:30:04Z",
      "side": 1,
      "message": "Same question in the other tasks",
      "revId": "e2f2231725fb9bff2701414bb1ebb50a59d0b64f",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "637fcd28_13b48625",
        "filename": "common-roles/check-resource/tasks/main.yml",
        "patchSetId": 1
      },
      "lineNbr": 91,
      "author": {
        "id": 800
      },
      "writtenOn": "2022-06-27T14:00:54Z",
      "side": 1,
      "message": "ditto",
      "parentUuid": "6a202929_527ce196",
      "revId": "e2f2231725fb9bff2701414bb1ebb50a59d0b64f",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "445831c7_e7714da2",
        "filename": "common-roles/check-resource/tasks/main.yml",
        "patchSetId": 1
      },
      "lineNbr": 136,
      "author": {
        "id": 673
      },
      "writtenOn": "2022-06-27T13:30:04Z",
      "side": 1,
      "message": "Wow,\nI am surprised! I did not expect that was possible to do this in Ansible ^^",
      "revId": "e2f2231725fb9bff2701414bb1ebb50a59d0b64f",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "9a11e993_65748514",
        "filename": "common-roles/check-resource/tasks/main.yml",
        "patchSetId": 1
      },
      "lineNbr": 136,
      "author": {
        "id": 800
      },
      "writtenOn": "2022-06-27T14:00:54Z",
      "side": 1,
      "message": "yes! in fact it is currently being used for BigIP and SPK and it has been working fine, so one of the benefits of this change is to also include this for the MCP workaround, as I said before.",
      "parentUuid": "445831c7_e7714da2",
      "revId": "e2f2231725fb9bff2701414bb1ebb50a59d0b64f",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    }
  ]
}