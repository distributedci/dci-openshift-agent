{
  "comments": [
    {
      "unresolved": false,
      "key": {
        "uuid": "925027b6_2cc465ba",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 4
      },
      "lineNbr": 0,
      "author": {
        "id": 724
      },
      "writtenOn": "2022-01-26T23:48:40Z",
      "side": 1,
      "message": "I haven\u0027t seem MCP pending after those tasks but approach looks good to me, just in case.\nThanks",
      "revId": "75bf015da3b7939ee90ff675d567092993e20f1b",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "45a2f01b_676f9a5c",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 4
      },
      "lineNbr": 0,
      "author": {
        "id": 821
      },
      "writtenOn": "2022-01-26T23:59:34Z",
      "side": 1,
      "message": "The idea is to be sure the nodes stay in a good shape after these tasks.\n\nTo avoid this case: https://www.distributed-ci.io/jobs/23643a1b-ce0c-4de0-b108-a762cb48f352/jobStates\nThe job was finished with success even if one node was in SchedulingDisabled state.",
      "parentUuid": "925027b6_2cc465ba",
      "revId": "75bf015da3b7939ee90ff675d567092993e20f1b",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "b76a1736_1479ac55",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 4
      },
      "lineNbr": 0,
      "author": {
        "id": 908
      },
      "writtenOn": "2022-01-27T00:19:35Z",
      "side": 1,
      "message": "Thanks for adding these validations, I have a question below.",
      "revId": "75bf015da3b7939ee90ff675d567092993e20f1b",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "537c321f_67fb8e28",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 4
      },
      "lineNbr": 0,
      "author": {
        "id": 724
      },
      "writtenOn": "2022-01-27T00:28:54Z",
      "side": 1,
      "message": "Yeah, I have seen this issue. What I found is that in that case the MC on the nodes is properly applied but at some time the kubelet on one node stops sending updates to the cluster. Having one node not schedulable makes the counts in the MCP be to look like \"updating\" because the number of nodes in the cluster does not match with the expected number of updated MCPs. \n\nEvery time you have a node out of service, the MCP status will be affected but that does not mean an actual issue with the MCs. That is what I have observed. But let\u0027s see if this helps. Thanks!",
      "parentUuid": "45a2f01b_676f9a5c",
      "revId": "75bf015da3b7939ee90ff675d567092993e20f1b",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "4eff4d19_58b353f0",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 4
      },
      "lineNbr": 0,
      "author": {
        "id": 673
      },
      "writtenOn": "2022-01-27T10:02:26Z",
      "side": 1,
      "message": "Exactly, even if a node error is not coming directly from MCs, it will be displayed in the MCP",
      "parentUuid": "537c321f_67fb8e28",
      "revId": "75bf015da3b7939ee90ff675d567092993e20f1b",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "4e2f85ca_22e8583d",
        "filename": "roles/operator-sriov/tasks/main.yml",
        "patchSetId": 4
      },
      "lineNbr": 137,
      "author": {
        "id": 908
      },
      "writtenOn": "2022-01-27T00:19:35Z",
      "side": 1,
      "message": "I like this but shouldn\u0027t we check only if the SR-IOV policies are applied? installing the operator does not triggers the MCP update if I remember correctly, only when applying a policy.",
      "revId": "75bf015da3b7939ee90ff675d567092993e20f1b",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    }
  ]
}