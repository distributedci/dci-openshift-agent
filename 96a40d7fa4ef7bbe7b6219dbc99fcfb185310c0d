{
  "comments": [
    {
      "unresolved": false,
      "key": {
        "uuid": "ac772c41_696577b0",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 24
      },
      "lineNbr": 0,
      "author": {
        "id": 938
      },
      "writtenOn": "2022-02-15T16:34:44Z",
      "side": 1,
      "message": "ocp-vanilla is working fine: https://www.distributed-ci.io/jobs/62ba332a-0c3f-4696-83ce-13c882ac0fe9/jobStates#01e1b858-c9b6-4bc0-93e3-54a3e591e895:file310 \n\nopenshift-vanilla-upgrade-4.8 is working fine as well: https://www.distributed-ci.io/jobs/f5e3fb67-07aa-4af5-b5f5-0a2306c014ba/jobStates#88e7fd4b-e424-4a38-b2c4-343070b7e740:file31 ",
      "revId": "96a40d7fa4ef7bbe7b6219dbc99fcfb185310c0d",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "beaa9dbf_e9d6930d",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 24
      },
      "lineNbr": 0,
      "author": {
        "id": 724
      },
      "writtenOn": "2022-02-15T17:12:43Z",
      "side": 1,
      "message": "Thanks Tatiana, this LGTM",
      "revId": "96a40d7fa4ef7bbe7b6219dbc99fcfb185310c0d",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "d11438f1_7344b643",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 24
      },
      "lineNbr": 0,
      "author": {
        "id": 908
      },
      "writtenOn": "2022-02-15T23:34:07Z",
      "side": 1,
      "message": "I see the last successful job actually never failed to use the work-around, that\u0027s funny. Just one question: let\u0027s assume an MC is getting applied, the first node gets stuck in SchedulingDisabled status, after the retries it gets uncordon, the MC will now move on to the other nodes, and it get\u0027s blocked too, the work-around only runs once in the node(s) that are SchedulingDisabled in the first check correct? \nOther than than I like how this is organized. Thanks.",
      "revId": "96a40d7fa4ef7bbe7b6219dbc99fcfb185310c0d",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "9f49acd2_f07f2b2c",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 24
      },
      "lineNbr": 0,
      "author": {
        "id": 908
      },
      "writtenOn": "2022-02-15T23:38:56Z",
      "side": 1,
      "message": "Also FYI, last failure is not related to this patch, the failure is on the bigip side.",
      "revId": "96a40d7fa4ef7bbe7b6219dbc99fcfb185310c0d",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "ce79209d_4ae5f7a2",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 24
      },
      "lineNbr": 0,
      "author": {
        "id": 800
      },
      "writtenOn": "2022-02-16T08:30:55Z",
      "side": 1,
      "message": "I would say this can be merged. By the way, the way we commented yesterday in the grooming to report the workarounds, I would include it here but maybe in a separate, new CR. I can handle it, as I am going to do the update for SPK/BigIP.",
      "revId": "96a40d7fa4ef7bbe7b6219dbc99fcfb185310c0d",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "fc0d1a04_0d3dd9d3",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 24
      },
      "lineNbr": 0,
      "author": {
        "id": 800
      },
      "writtenOn": "2022-02-16T08:30:55Z",
      "side": 1,
      "message": "Yep, that\u0027s it, it\u0027s a think that I already commented to Tatiana and that kind of retry to uncordon the new node in SchedulingDisabled status may be included in the future if needed. This is a feature that, for example, I\u0027ve already included in BigIP/SPK PRs. Here, we can include it or not, but for the moment, I guess it\u0027s enough to continue.\n@Tatiana wdyt? From my side, it\u0027s fine to merge this without including this kind of retry, we can add it later if needed",
      "parentUuid": "d11438f1_7344b643",
      "revId": "96a40d7fa4ef7bbe7b6219dbc99fcfb185310c0d",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "ef4c8804_400badbe",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 24
      },
      "lineNbr": 0,
      "author": {
        "id": 938
      },
      "writtenOn": "2022-02-16T08:55:16Z",
      "side": 1,
      "message": "\u003e I see the last successful job actually never failed to use the work-around, that\u0027s funny.\n\nIt did fail once, not on the last patchset though: https://www.distributed-ci.io/jobs/ff289f1f-9cf9-417b-9885-d48413771d53/jobStates#72c152d5-b53c-4e6a-9177-4c67a74c175a:file30 Somehow MachineConfigPool object didn\u0027t exist.\n\n\u003e let\u0027s assume an MC is getting applied, the first node gets stuck in SchedulingDisabled status, after the retries it gets uncordon, the MC will now move on to the other nodes, and it get\u0027s blocked too, the work-around only runs once in the node(s) that are SchedulingDisabled in the first check correct? \n\nI\u0027ve never seen that situation. Usually, we have one last worker with one last MC to be applied. But if that would be the case, we could always modify the workaround to uncordon in a loop :)",
      "parentUuid": "d11438f1_7344b643",
      "revId": "96a40d7fa4ef7bbe7b6219dbc99fcfb185310c0d",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "22b8e0c6_a8dcb874",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 24
      },
      "lineNbr": 0,
      "author": {
        "id": 938
      },
      "writtenOn": "2022-02-16T08:55:16Z",
      "side": 1,
      "message": "Thank you @Beto for the refactoring.\n\nThank you for the reviews @Manuel and @Ramon.\n\nI\u0027m going to merge it as the first step because at least the change does no harm, and the workaround worked once. We could always update it. ",
      "revId": "96a40d7fa4ef7bbe7b6219dbc99fcfb185310c0d",
      "serverId": "6c1dc8ef-8b94-40e4-bd83-c2359d45ecc0"
    }
  ]
}